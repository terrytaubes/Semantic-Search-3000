Algorithmic bias occurs when data used to teach a machine learning system reflects implicit values of humans involved in that data collection, selection, or use. Algorithmic bias has been identified and critiqued for its impact on search engine results, social media platforms, privacy, and racial profiling. In search results, this bias can create results reflecting racist or sexist social biases despite the presumed neutrality of the data. The study of algorithmic bias is most concerned with algorithms that reflect "systematic and unfair" discrimination.
As algorithms expand their ability to organize society, politics, institutions and individuals, bias has become a subject of critique among sociologists concerned with the ways unanticipated output and manipulation can impact the physical world. Because algorithms are often considered to be neutral and unbiased, they can inaccurately project greater authority than human expertise. Deferring to algorithms can also displace human responsibility for selected outcomes. Nonetheless, bias can enter into algorithmic systems as a result of pre-existing cultural, social or institutional expectations; as a result of technical limitations of their design; or through use in unanticipated contexts or by audiences not considered in their initial design.
Algorithmic bias has been discovered or theorized in cases ranging from election outcomes to the spread of online hate speech. Problems in understanding, researching, and discovering algorithmic bias may come from the proprietary nature of algorithms, which are typically treated as trade secrets. Even with full transparency, understanding algorithms can be difficult owing to issues of complexity in their design, and because not every permutation of an algorithms input or output is able to be considered or reproduced. In most cases, even within a single use-case such as a website or app, there is no single "algorithm" to examine, but a vast network of interrelated programs, data inputs, even between users of the same service.


== Definitions ==

Algorithms are difficult to define, but may be generally understood as sets of instructions within computer programs that determine how these programs read, collect, process and analyze data to generate some form of analysis and output. Contemporary computers are able to process millions of these algorithmic instructions per second, which has boosted the design and adoption of technologies such as machine learning and artificial intelligence. By analyzing and processing data, algorithms drive search engines, social media websites, recommendation engines, online retail, online advertising, and more.
Contemporary social scientists are concerned with algorithmic processes embedded into hardware and software applications in order to understand their political effects, and to question the underlying assumptions of their neutrality. The term algorithmic bias is used to describe systematic and repeatable errors that create unfair outcomes, i.e., generating one result for certain users and another result for others. For example, a credit score algorithm may deny a loan based on certain factors without being unfair. If that algorithm allows one loan but denies another to sets of nearly identical users, and if this behavior can be repeated across multiple occurrences, an algorithm can be described as biased. This bias may be intentional or unintentional.


== Methods ==
Bias can be introduced to an algorithm in several ways. During the assemblage of a database, data must be collected, digitized, adapted, and entered according to human-assisted cataloging criteria. Next, in the design of the algorithm, programmers assign certain priorities, or hierarchies, in how programs assess and sort that data; this includes human decisions about which data is categorized and which is discarded. Some algorithms collect their own data based on human-selected criteria, which can reflect the bias of human users. Others may practice reinforcing stereotypes and preferences as they process and display "relevant" data for human users, as in selecting information based on previous choices of a user.
Beyond the data itself, however, bias can also emerge as a result of the design of algorithmic functions. In sorting processes that determine the allocation of resources or scrutiny, as in determining school placements, or classification and identification processes that may inadvertently discriminate against a category when assigning risk, as in credit scores. In processing associations, such as recommendation engines or inferred marketing traits, algorithms may be flawed in ways that, for example, reveal personal information; or inclusion and exclusion criteria may have unanticipated outcomes for search results, such as in flight recommendation software.


== History ==


=== Early critiques ===
The earliest computer programs reflected simple, human-derived operations, and were deemed to functioning when they completed those operations. Artificial Intelligence pioneer Joseph Weizenbaum wrote that such programs are therefore understood to "embody law." Weizenbaum describes early, simple computer programs changing perceptions of machines from transferring power to transferring information. However, he noted that machines might transfer information with unintended consequences if there are errors in details provided to the machine, and if users interpret data in intuitive ways that cannot be formally communicated to, or from, a machine. Weizenbaum stated that all data fed to a machine must reflect "human decisionmaking processes" which have been translated into rules for the computer to follow. To do this, Weizenbaum asserts that programmers "legislate the laws for a world one first has to create in imagination," and as a result, computer simulations can be built on models with incomplete or incorrect data. Weizenbaum compared the results of such decisions to a tourist in a country who can make "correct" decisions through a coin toss, but has no basis of understanding how or why the decision was made.


=== Contemporary critiques ===
The complexity of analyzing algorithmic bias has grown alongside the complexity of programs and their design. Decisions made by one designer, or team of designers, may be obscured among the many aspects of code created for a single program; over time these decisions and their impact may be forgotten and taken as natural results of the program's output. These biases can create new patterns of behavior, or "scripts," in relationship to specific technologies as the code interacts with other elements of society. Biases may also impact how society shapes itself around the data points that algorithms require.
The decisions of algorithmic programs can be weighed more heavily than the decisions of the human beings they are meant to assist, a process described by author Clay Shirky as "algorithmic authority." Shirky uses the term to describe "the decision to regard as authoritative an unmanaged process of extracting value from diverse, untrustworthy sources," such as search results. This neutrality can also be presented through the language frames around the results presented to the public. For example, a list of news items selected and presented as "trending" or "popular" may be weighed based on significantly wider criteria than their popularity.
Because of their convenience and authority, algorithms are theorized as a means of delegating responsibility in decision making away from humans. This can have the effect of reducing alternative options, compromises, or flexibility. Sociologist Scott Lash has invited the critique of algorithms as a new form of "generative power" in that they are a virtual means of generating actual ends.


== Types ==


=== Pre-existing ===

Pre-existing bias in an algorithm as a consequence of social and institutional ideologies. Such ideas may reflect personal biases of individual designers or programmers, or can reflect social, institutional, or cultural assumptions. In both cases, such prejudices can be explicit and conscious, or implicit and unconscious. Poorly selected input data will influence the outcomes created by machines. In a critical view, encoding pre-existing bias into software can preserve social and institutional bias, and replicate it into all possible uses of the algorithm into the future.
An example of this form of bias is the British Nationality Act Program, designed to automate the evaluation of new UK citizens after the 1981 British Nationality Act. The program accurately reflected the tenets of the law, which stated that "a man is the father of only his legitimate children, whereas a woman is the mother of all her children, legitimate or not." By attempting to appropriately articulate this logic into an algorithmic process, the BNAP inscribed the logic of the British Nationality Act into its algorithm.


=== Technical ===
Technical bias emerges through limitations of a program, computational power, its design, or other constraint on the system. Such bias can also be a restraint of design, for example, a search engine that shows three results per screen can be understood to privilege the top three results slightly more than the next three, as in airline price displays. Flaws in random number generation can also introduce bias into results.
A decontextualized algorithm uses unrelated information to sort results, for example, a flight pricing algorithm that sorts results by alphabetical order would seem biased in favor of American Airlines over United Airlines. The opposite may also be true, in that data may be collected without crucial external context, as in the case of facial recognition software in surveillance cameras being evaluated by long-distance staff, or non-human algorithms, with no awareness of the spaces surrounding the camera's field of vision.
Lastly, technical bias can be created simply through the attempt to formalize decisions into concrete steps on the assumption that human behavior will correlate. For example, software that weighs data points to determine whether a defendant should accept a plea bargain, while ignoring the impact of emotion on a jury.


=== Emergent ===
Emergent bias is the result of the use and reliance on algorithms across various contexts. New forms of knowledge may be discovered, such as drug or medical breakthroughs, new laws, business models, or shifting cultural norms, which algorithms aren't updated to consider. This creates an unintended exclusion of groups through technology, without clear outlines of authorship or personal responsibility. Similarly, problems may emerge when training data, i.e., the samples "fed" to a machine by which it models certain conclusions, do not align with uses that algorithm encounters in the real world.
In 1990, an example of emergent bias was identified in the software used to place medical students into residencies, the National Residency Match Program (NRMP). The algorithm was designed at a time when few married couples would seek residencies at the same time. As more women entered medical schools, more students were likely to request a residency alongside their partners. The process is a list of preferences across the US, which is then sorted and assigned when a hospital and an applicant agree to a match. In the case of married couples where both sought residencies, the algorithm weighed a "lead members" residency location choices first. Once it identified an optimum placement, it removed distant locations from their partner's preferences, leaving only preferred locations in the same city as the partner. The result was a frequent assignment of lower-preference schools to the second partner.
Emergent bias can also be the result of an algorithm's use by unanticipated audiences, such as machines that rely on an ability to read, write, or understand numbers. Likewise, certain metaphors may not carry across different populations or skill sets. For example, the British National Act Program was created as a proof of concept by computer scientists and immigration lawyers to evaluate suitability for British citizenship. The designers are therefore removed from their potential users, whose understanding of both software and immigration law would likely be unsophisticated; the agents administering the questions would not be aware of alternative pathways to citizenship outside of the software, and shifting case law and legal interpretations would lead the algorithm to outdated results.
The concerns around emergent bias is that they may be compounded as technologies are integrated into society. For example, users with vision impairments may not be able to use an ATM, but can go to a bank branch. If bank branches begin to close because of the ubiquity of ATMs, then this begins to exclude vision-impaired users from banking, an unintended consequence of a technology.


== Examples ==


=== College admissions ===
An early example of algorithmic bias resulted in as many as 60 women and ethnic minorities denied entry to St. George's Hospital Medical School per year from 1982 to 1986, based on implementation of a new computer-guidance assessment system that denied entry to women and men with "foreign-sounding names" based on historical, human, trends in admissions. Other examples include the display of higher-paying jobs to male applicants on job search websites.


=== Plagiarism detection ===
The plagiarism-detection software Turnitin, which compares student-written texts to information found online and returns a probability that the student's work is copied. Because the software compares strings of text, it is more likely to identify non-native speakers of English than native speakers, who might be better able to adapt individual words and break up strings of plagiarized text or obscure them through synonyms.


=== Facial recognition ===
Surveillance camera software may be considered inherently political as it requires algorithms to identify and flag normal from abnormal behaviors, and to determine who belongs in certain locations at certain times. A 2002 analysis of facial recognition software used to identify individuals in CCTV images found several examples of bias. Software was assessed at identifying men more frequently than women, older people more frequently than younger people, and that Asians, African-Americans and other races were more identifiable than whites.


=== Social media ===
In 2017 a Facebook algorithm designed to remove online hate speech was found to advantage white men over black children assessing objectionable content, according to internal Facebook documents. The algorithm, which is a blend of computer programs and human content reviews, was created to protect broad categories rather than specific subsets of categories. For example, posts denouncing "Muslims" would be blocked, while posts denouncing "Radical Muslims" would be allowed. An unanticipated outcome of the algorithm is to allow hate speech against black children, because they denounce the "children" subset of blacks, rather than "all blacks," whereas "all white men" would trigger a block, because whites and males are not considered "subsets." Facebook was also found to allow ad purchasers to target "Jew haters" as a category of users, which Facebook said was an inadvertent outcome of algorithms used in assessing and categorizing data. The company's design also allowed ad buyers to block African-Americans from seeing ads.


== Impact ==


=== Commercial influences ===
Corporate algorithms could be skewed to invisibly favor financial arrangements or agreements between companies, without the knowledge of a user who may mistake the algorithm as being impartial. For example, American Airlines created a flight-finding algorithm in the 1980s, which presented a range of flights from various airlines to customers, but weighed factors that heavily boosted the rank of its own flights, regardless of price or convenience. In testimony to the United States Congress, the president of the airline stated outright that the system was created with the intention of gaining competitive advantage through preferential treatment.
In their 1998 paper describing Google, adopted a policy of transparency in search results regarding paid placement, arguing that “advertising-funded search engines will be inherently biased towards the advertisers and away from the needs of the consumers.” This bias would be an "invisible" manipulation of the user.


=== Voting behavior ===
A series of studies of undecided voters in the US and in India found that search engine results were able to shift voting outcomes by about 20%. The researchers concluded that candidates have "no means of competing" if an algorithm, with or without intent, boosted page listings for a rival candidate.
Facebook users who saw messages related to voting were more likely to vote themselves, with one randomized trial of Facebook users showing an increased effect of 340,000 votes among users, and friends of users, who saw pro-voting messages in 2010. The legal scholar Jonathan Zittrain has warned that this could create a "digital gerrymandering" effect in elections, "the selective presentation of information by an intermediary to meet its agenda, rather than to serve its users," if intentionally manipulated.


=== Gender discrimination ===
In 2012, the department store franchise Target was cited for gathering data points to infer when women customers were pregnant, even if they hadn't announced it, and then sharing that information with marketing partners. Because the data had been predicted, rather than directly observed or reported, the company had no legal obligation to protect the privacy of those customers.
Web search algorithms have also been accused of bias. Google's results may prioritize pornographic content in search terms related to sexuality, for example, "lesbian." This bias extends to the search engine surfacing popular but sexualized content in neutral searches, as in "Top 25 Sexiest Women Athletes" articles displayed as first-page results in searches for "women atheletes." In 2017 Google announced plans to curb search results that surfaced hate groups, racist views, child abuse and pornography, and other upsetting and offensive content.


=== Racial discrimination ===
Algorithms have been criticized as a method for obscuring racial prejudices in decision-making. Lisa Nakamura has noted that census machines were among the first to adopt the punch-card processes that lead to contemporary computing, and that their use as categorization and sorting machines for race has been long established and socially tolerated.
One example is the use of risk assessments in criminal sentencing and parole hearings, an algorithmically generated score intended to reflect the risk that a suspect or prisoner will repeat a crime. From 1920 until 1970, the nationality of a suspect's father was a consideration in such risk assessments. Today, these scores are shared with judges in Arizona, Colorado, Delaware, Kentucky, Louisiana, Oklahoma, Virginia, Washington, and Wisconsin. An independent investigation by ProPublica found that the scores were inaccurate 80% of the time, and disproportionately skewed to suggest blacks to be at risk of relapse, 77% more often than whites.
In 2015, Google apologized when black users complained that an image-identification algorithm in its Photos application identified them as gorillas. In 2010, Nikon cameras were criticized when image-recognition algorithms consistently asked Asian users if they were blinking. Such examples are the product of bias in biometric data sets. Biometric data is drawn from aspects of the body, including racial features either observed or inferred, which can then be transferred into data points.
Biometric data about race may also be inferred, rather than observed. For example, a 2012 study showed that names commonly associated with blacks were more likely to yield search results implying arrest records, regardless of any police record of that individual's name.


=== Sexual discrimination ===
In 2011, users of the gay hookup app Grindr reported that the app was linked to sex-offender lookup apps in the Android app store's recommendation algorithm. Writer Mike Ananny criticized this association in The Atlantic, arguing that such associations further stigmatized gay men and may discourage closeted men to maintain secrecy. A 2009 incident with online retailer Amazon saw 57,000 books de-listed after a shift in the algorithm expanded its "adult content" blacklist for pornographic works to any books addressing sexuality or gay themes, for example, the critically acclaimed novel Brokeback Mountain.


== Challenges ==
Several challenges impede the study of large-scale algorithmic bias, hindering the application of academically rigorous studies and public understanding.


=== Lack of transparency ===
Commercial algorithms are proprietary, and may be treated as trade secrets. This protects companies, such as a search engine, in cases where a transparent algorithm for ranking results would reveal techniques for manipulating the service. This makes it difficult for researchers to conduct interviews or analysis to discover how algorithms function. It can also be used to obscure possible unethical methods used in producing or processing algorithmic output. The closed nature of the code is not the only concern, however; as a certain degree of obscurity is protected by the complexity of contemporary programs, and the inability to know every permutations of a code's input or output.
Social scientist Bruno Latour has identified this process as blackboxing, a process in which "scientific and technical work is made invisible by its own success. When a machine runs efficiently, when a matter of fact is settled, one need focus only on its inputs and outputs and not on its internal complexity. Thus, paradoxically, the more science and technology succeed, the more opaque and obscure they become." Others have critiqued the black box metaphor, suggesting that current algorithms are not one black box, but a network of interconnected ones.


=== Complexity ===
Algorithmic processes are complex, often exceeding the understanding of the people who use them. Large-scale operations may not be understood even by those involved in creating them. The social media site Facebook factored in at least 100,000 data points to determine the layout of a user's social media feed in 2013. Furthermore, large teams of programmers may operate in relative isolation from one another, and be unaware of the cumulative effects of small decisions with nested sections of sprawling algorithmic processes. Not all code is original, and may be borrowed from other libraries, creating a complicated set of relationships between data processing and data input systems.


=== Lack of data about sensitive categories ===
A significant barrier to understanding tackling bias in practice is that categories, such as demographics of individuals protected by anti-discrimination law, are often not explicitly held by those collecting and processing data. In some cases, there is little opportunity to collect this data explicitly, such as in device fingerprinting, ubiquitous computing and the Internet of Things. In other cases, the data controller may not wish to collect such data for reputational reasons, or because it represents a heightened liability and security risk. It may also be the case that, at least in relation to the General Data Protection Regulation, such data falls under the 'special category' provisions (Article 9), and therefore comes with more restrictions on potential collection and processing.
Furthemore, algorithmic bias does not only relate to protected categories, but can also concern something less easily observable or codifiable, such as political viewpoints. In these cases, there is rarely an easily accessible or non-controversial ground truth, and 'debiasing' such a system becomes considerably more tricky.


=== Rapid pace of change ===
Personalization of algorithms based on user interactions such as clicks, time on site, and other metrics, can confuse attempts to understand them. One unidentified streaming radio service reported it had five unique music-selection algorithms it selected for its users based on behavior. This creates widely disparate experiences of the same streaming product between different users. Companies also run frequent A/B tests to fine-tune algorithms based on user response. For example, the search engine Bing can run up to ten million subtle variations of its service per day, segmenting the experience of an algorithm between users, or among the same users.


=== Rapid pace of dissemination ===
Computer programs and systems can quickly spread among users, embedding biased algorithms into broader society before their impact can be recognized or remedied.


== References ==